
R version 4.2.0 (2022-04-22) -- "Vigorous Calisthenics"
Copyright (C) 2022 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> rm(list = ls())
> gc()
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 275579 14.8     666002 35.6   456470 24.4
Vcells 462977  3.6    8388608 64.0  1820341 13.9
> 
> # Libraries
> library(parallel) # For parallel computation
> library(doParallel) # For parallel computation
Loading required package: foreach
Loading required package: iterators
> library(foreach) # For parallel computation
> library(BayesTree) # For BART
> library(spNNGP) # For NNGP
Loading required package: coda
Loading required package: Formula
Loading required package: RANN
> library(BASS) # For BASS
> 
> # Read in
> load("data/flood_data.RData")
> 
> # Clusters and seed
> nCores <- 5
> mySeed <- 123
> 
> # Read in indices for test data
> n <- nrow(coords)
> indexTest <- readRDS("results/test_points.RDS")
> nTest <- length(indexTest)
> storms <- 1:5
> nSubj <- length(storms)
> stormsTest <- 6:10
> nTestSubj <- length(stormsTest)
> 
> Y <- lapply(storms, \(i) out[i, ])
> X <- lapply(storms, \(i) {
+   Xstorm <- matrix(rep(unlist(inputs[i, ]), n), ncol = 5, byrow = TRUE)
+   Xelev <- coords$elev_meters #[indexTrain]
+   X <- cbind(Xstorm, Xelev)
+   colnames(X) <- c(colnames(inputs), "elev")
+   return(X)
+ })
> S <- coords[ , 1:2]
> 
> YTest <- lapply(stormsTest, \(i) out[i, indexTest])
> XTest <- lapply(stormsTest, \(i) {
+   Xstorm <- matrix(rep(unlist(inputs[i, ]), n), ncol = 5, byrow = TRUE)
+   Xelev <- coords$elev_meters[indexTest]
+   X <- cbind(Xstorm, Xelev)
+   colnames(X) <- c(colnames(inputs), "elev")
+   return(X)
+ })
Warning messages:
1: In cbind(Xstorm, Xelev) :
  number of rows of result is not a multiple of vector length (arg 2)
2: In cbind(Xstorm, Xelev) :
  number of rows of result is not a multiple of vector length (arg 2)
3: In cbind(Xstorm, Xelev) :
  number of rows of result is not a multiple of vector length (arg 2)
4: In cbind(Xstorm, Xelev) :
  number of rows of result is not a multiple of vector length (arg 2)
5: In cbind(Xstorm, Xelev) :
  number of rows of result is not a multiple of vector length (arg 2)
> STest <- coords[indexTest, 1:2]
> 
> ##############
> #### BART ####
> ##############
> 
> bart_obj <- bart(do.call('rbind', X), do.call('c', Y), do.call('rbind', XTest))


Running BART with numeric y

number of trees: 200
Prior:
	k: 2.000000
	degrees of freedom in sigma prior: 3
	quantile in sigma prior: 0.900000
	power and base for tree prior: 2.000000 0.950000
	use quantiles for rule cut points: 0
data:
	number of training observations: 248595
	number of test observations: 248595
	number of explanatory variables: 6


Cutoff rules c in x<=c vs x>c
Number of cutoffs: (var: number of possible c):
(1: 100) (2: 100) (3: 100) (4: 100) (5: 100) 
(6: 100) 


Running mcmc loop:
iteration: 100 (of 1100)
iteration: 200 (of 1100)
iteration: 300 (of 1100)
iteration: 400 (of 1100)
iteration: 500 (of 1100)
iteration: 600 (of 1100)
iteration: 700 (of 1100)
iteration: 800 (of 1100)
iteration: 900 (of 1100)
iteration: 1000 (of 1100)
iteration: 1100 (of 1100)
time for loop: 20946

Tree sizes, last iteration:
3 2 3 3 3 4 3 3 2 4 3 2 2 2 2 3 6 2 2 2 
3 4 7 2 2 3 2 2 3 2 3 1 3 3 2 2 2 2 3 2 
2 2 2 3 2 2 5 3 1 4 2 2 2 4 2 2 3 2 4 2 
3 2 3 3 2 2 2 2 4 2 1 2 2 2 2 2 2 2 1 3 
2 2 4 3 2 2 2 2 5 2 2 2 3 2 2 4 2 2 3 3 
2 2 2 3 3 2 2 2 2 2 2 3 2 2 3 2 2 2 1 2 
2 4 2 2 2 2 2 1 2 2 2 2 2 2 2 3 2 2 2 2 
3 2 2 2 2 4 2 1 5 2 2 3 3 3 2 2 2 2 2 2 
2 2 3 2 3 3 2 2 2 3 2 2 2 2 3 2 2 2 2 2 
2 3 2 2 2 2 2 5 2 3 5 5 2 2 2 3 2 1 3 2 
Variable Usage, last iteration (var:count):
(1: 32) (2: 38) (3: 31) (4: 42) (5: 32) 
(6: 110) 
DONE BART 11-2-2014

> bartPreds <- bart_obj$yhat.test.mean
> bartLower <- apply(bart_obj$yhat.test, 1, quantile, 0.025)
> bartUpper <- apply(bart_obj$yhat.test, 1, quantile, 0.975)
> 
> if (file.exists(".RData")) {
+   file.remove(".RData")
+ }
[1] TRUE
> gc()
            used   (Mb) gc trigger    (Mb)   max used    (Mb)
Ncells    426736   22.8    1582624    84.6    1582624    84.6
Vcells 701040185 5348.6 2269599521 17315.7 2269506253 17315.0
> 
> bart <- list(preds = bartPreds, lower = bartLower, upper = bartUpper)
> saveRDS(bart, "results/flood_results_bart.RDS")
> 
> ##############
> #### NNGP ####
> ##############
> 
> nIter <- 2500
> cov.model <- "exponential"
> starting <- list("phi"=5, "sigma.sq"=1, "tau.sq"=0.2)
> tuning <- list("phi"=5, "sigma.sq"=2, "tau.sq"=2)
> priors <- list("phi.Unif"=c(1,20), "sigma.sq.IG"=c(1, 1), "tau.sq.IG"=c(1, 1))
> 
> # Remove global attributes from X train and test
> X <- lapply(storms, \(i) coords$elev_meters)
> XTest <- lapply(stormsTest, \(i) coords$elev_meters)
> 
> cl <- makeCluster(nCores)
> registerDoParallel(cl)
> strt <- Sys.time()
> set.seed(mySeed)
> nngp_obj <- foreach(i = storms, .packages = "spNNGP") %dopar% spNNGP(Y[[i]] ~ X[[i]], coords=S, 
+                                                                      starting=starting, method="latent", 
+                                                                      n.neighbors=10, tuning=tuning, 
+                                                                      priors=priors, cov.model=cov.model,
+                                                                      n.samples=nIter, n.omp.threads=1, fit.rep=TRUE)
> final.time <- Sys.time() - strt
> stopCluster(cl)
> if (file.exists(".RData")) {
+   file.remove(".RData")
+ }
> gc()
             used    (Mb) gc trigger    (Mb)   max used    (Mb)
Ncells     496735    26.6    1582624    84.6    1582624    84.6
Vcells 1945418573 14842.4 2723599425 20779.5 2269506253 17315.0
> 
> # Extract parameter estimates and credible intervals
> means <- apply(sapply(1:nSubj, function(i) apply(nngp_obj[[i]]$p.theta.samples, 2, mean)), 1, mean)
> lowers <- apply(sapply(1:nSubj, function(i) apply(nngp_obj[[i]]$p.theta.samples, 2, quantile, .025)), 1, mean)
> uppers <- apply(sapply(1:nSubj, function(i) apply(nngp_obj[[i]]$p.theta.samples, 2, quantile, .975)), 1, mean)
> betas <- sapply(1:nSubj, function(i) mean(nngp_obj[[i]]$p.beta.samples[ ,2]))
> nngpParams <- as.data.frame(rbind(means, lowers, uppers))
> nngpParams <- data.frame(nngpParams, beta = c(mean(betas), quantile(betas, c(.025, .975))))
> rownames(nngpParams) <- c("mean", "lower", "upper")
> 
> # Aggregate predictions for test points
> nngpPreds <- apply(sapply(1:nSubj, \(i) nngp_obj[[i]]$y.hat.quant[indexTest, 1]), 1, mean)
> nngpLower <- apply(sapply(1:nSubj, \(i) nngp_obj[[i]]$y.hat.quant[indexTest, 2]), 1, mean)
> nngpUpper <- apply(sapply(1:nSubj, \(i) nngp_obj[[i]]$y.hat.quant[indexTest, 3]), 1, mean)
> 
> nngp <- list(nngpParams = nngpParams, 
+ 	     nngpPreds = nngpPreds,
+ 	     nngpLower = nngpLower,
+ 	     nngpUpper = nngpUpper) 
> saveRDS(nngp, "results/flood_results_nngp.RDS")
> 
> 
> ##############
> #### BASS ####
> ##############
> 
> nStorms <- length(storms) + length(stormsTest)
> nTestSubj <- length(stormsTest)
> n <- nrow(coords)
> inputs <- inputs[1:nStorms, ]
> out <- out[1:nStorms, ]
> gc()
             used    (Mb) gc trigger    (Mb)   max used    (Mb)
Ncells     503070    26.9    1582624    84.6    1582624    84.6
Vcells 1747039098 13328.9 2723599425 20779.5 2269506253 17315.0
> 
> set.seed(mySeed)
> model <- bassPCA(inputs[-stormsTest, ], out[-stormsTest, ], n.pc=2, n.cores=1)
MCMC Start #-- Oct 22 10:29:54 PM --# nbasis: 0 
MCMC iteration 1000 #-- Oct 22 10:29:55 PM --# nbasis: 1 
MCMC iteration 2000 #-- Oct 22 10:29:55 PM --# nbasis: 1 
MCMC iteration 3000 #-- Oct 22 10:29:56 PM --# nbasis: 2 
MCMC iteration 4000 #-- Oct 22 10:29:56 PM --# nbasis: 2 
MCMC iteration 5000 #-- Oct 22 10:29:57 PM --# nbasis: 1 
MCMC iteration 6000 #-- Oct 22 10:29:57 PM --# nbasis: 2 
MCMC iteration 7000 #-- Oct 22 10:29:58 PM --# nbasis: 2 
MCMC iteration 8000 #-- Oct 22 10:29:58 PM --# nbasis: 2 
MCMC iteration 9000 #-- Oct 22 10:29:59 PM --# nbasis: 1 
MCMC iteration 10000 #-- Oct 22 10:29:59 PM --# nbasis: 1 
MCMC Start #-- Oct 22 10:29:59 PM --# nbasis: 0 
MCMC iteration 1000 #-- Oct 22 10:30:00 PM --# nbasis: 1 
MCMC iteration 2000 #-- Oct 22 10:30:00 PM --# nbasis: 1 
MCMC iteration 3000 #-- Oct 22 10:30:01 PM --# nbasis: 0 
MCMC iteration 4000 #-- Oct 22 10:30:01 PM --# nbasis: 1 
MCMC iteration 5000 #-- Oct 22 10:30:02 PM --# nbasis: 1 
MCMC iteration 6000 #-- Oct 22 10:30:02 PM --# nbasis: 1 
MCMC iteration 7000 #-- Oct 22 10:30:03 PM --# nbasis: 0 
MCMC iteration 8000 #-- Oct 22 10:30:03 PM --# nbasis: 1 
MCMC iteration 9000 #-- Oct 22 10:30:04 PM --# nbasis: 2 
MCMC iteration 10000 #-- Oct 22 10:30:04 PM --# nbasis: 0 
> predictions <- predict(model, inputs[stormsTest, ])
> bassPreds <- apply(predictions, 2:3, mean)
> bassLower <- apply(predictions, 2:3, quantile, 0.025)
> bassUpper <- apply(predictions, 2:3, quantile, 0.975)
> 
> bass <- list(bassPreds = bassPreds,
+ 	     bassLower = bassLower,
+ 	     bassUpper = bassUpper)
> saveRDS(bass, "results/flood_results_bass.RDS")
> 
> mspe <- sapply(1:nTestSubj, \(i) mean((bassPreds[i, indexTest] - out[stormsTest[i], indexTest])^2) )
> mean(mspe)
[1] 6.658316
> 
> pct <- sapply(1:nTestSubj, \(i) mean((bassPreds[i, indexTest] > 4) == (out[stormsTest[i], indexTest] > 4)) )
> 1-mean(pct)
[1] 0.206841
> 
> proc.time()
     user    system   elapsed 
21080.431    89.327 22460.502 

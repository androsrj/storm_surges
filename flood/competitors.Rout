
R version 4.2.0 (2022-04-22) -- "Vigorous Calisthenics"
Copyright (C) 2022 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> rm(list = ls())
> gc()
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 273690 14.7     660857 35.3   451905 24.2
Vcells 459126  3.6    8388608 64.0  1800280 13.8
> 
> # Libraries
> library(parallel) # For parallel computation
> library(doParallel) # For parallel computation
Loading required package: foreach
Loading required package: iterators
> library(foreach) # For parallel computation
> library(BayesTree) # For BART
> library(spNNGP) # For NNGP
Loading required package: coda
Loading required package: Formula
Loading required package: RANN
> library(BASS) # For BASS
> 
> # Read in
> load("data/flood_data.RData")
> 
> # Clusters and seed
> nCores <- 5
> mySeed <- 123
> 
> # Read in indices for test data
> n <- nrow(coords)
> indexTest <- readRDS("results/test_points.RDS")
> nTest <- length(indexTest)
> storms <- 1:5
> nSubj <- length(storms)
> stormsTest <- 6:10
> nTestSubj <- length(stormsTest)
> 
> Y <- lapply(storms, \(i) out[i, ])
> X <- lapply(storms, \(i) {
+   Xstorm <- matrix(rep(unlist(inputs[i, ]), n), ncol = 5, byrow = TRUE)
+   Xelev <- coords$elev_meters #[indexTrain]
+   X <- cbind(Xstorm, Xelev)
+   colnames(X) <- c(colnames(inputs), "elev")
+   return(X)
+ })
> S <- coords[ , 1:2]
> 
> YTest <- lapply(stormsTest, \(i) out[i, indexTest])
> XTest <- lapply(stormsTest, \(i) {
+   Xstorm <- matrix(rep(unlist(inputs[i, ]), nTest), ncol = 5, byrow = TRUE)
+   Xelev <- coords$elev_meters[indexTest]
+   X <- cbind(Xstorm, Xelev)
+   colnames(X) <- c(colnames(inputs), "elev")
+   return(X)
+ })
> STest <- coords[indexTest, 1:2]
> 
> ##############
> #### BART ####
> ##############
> 
> strt <- Sys.time()
> bart_obj <- bart(do.call('rbind', X), do.call('c', Y), do.call('rbind', XTest))


Running BART with numeric y

number of trees: 200
Prior:
	k: 2.000000
	degrees of freedom in sigma prior: 3
	quantile in sigma prior: 0.900000
	power and base for tree prior: 2.000000 0.950000
	use quantiles for rule cut points: 0
data:
	number of training observations: 248595
	number of test observations: 4970
	number of explanatory variables: 6


Cutoff rules c in x<=c vs x>c
Number of cutoffs: (var: number of possible c):
(1: 100) (2: 100) (3: 100) (4: 100) (5: 100) 
(6: 100) 


Running mcmc loop:
iteration: 100 (of 1100)
iteration: 200 (of 1100)
iteration: 300 (of 1100)
iteration: 400 (of 1100)
iteration: 500 (of 1100)
iteration: 600 (of 1100)
iteration: 700 (of 1100)
iteration: 800 (of 1100)
iteration: 900 (of 1100)
iteration: 1000 (of 1100)
iteration: 1100 (of 1100)
time for loop: 19966

Tree sizes, last iteration:
2 2 3 3 3 1 2 2 2 2 1 2 2 3 2 1 3 3 2 2 
2 2 4 2 2 2 3 2 2 3 3 2 3 3 1 3 3 3 2 2 
3 5 2 3 2 3 3 3 2 1 2 2 2 3 2 2 4 3 2 1 
2 3 3 2 3 2 3 4 2 4 3 2 4 3 4 2 3 2 2 4 
2 2 2 4 2 3 2 1 2 2 2 2 3 3 2 4 4 2 2 3 
4 4 2 4 3 1 3 2 2 3 2 2 2 2 2 5 2 2 3 2 
2 2 2 2 2 4 3 2 2 2 4 1 2 4 3 2 2 3 2 2 
3 2 2 3 4 3 3 2 2 2 2 1 2 2 2 3 3 2 2 5 
2 4 2 3 2 3 3 1 2 3 1 2 2 2 2 5 2 3 3 2 
2 2 2 4 2 2 2 2 3 1 2 2 2 2 2 4 5 2 2 2 
Variable Usage, last iteration (var:count):
(1: 35) (2: 33) (3: 34) (4: 33) (5: 45) 
(6: 114) 
DONE BART 11-2-2014

> bartPreds <- bart_obj$yhat.test.mean
> bartLower <- apply(bart_obj$yhat.test, 2, quantile, 0.025)
> bartUpper <- apply(bart_obj$yhat.test, 2, quantile, 0.975)
> 
> bartTime <- Sys.time() - strt
> if (file.exists(".RData")) {
+   file.remove(".RData")
+ }
> gc()
            used   (Mb) gc trigger    (Mb)   max used   (Mb)
Ncells    427519   22.9    1584974    84.7    1584974   84.7
Vcells 455719348 3476.9 1323883198 10100.5 1025544321 7824.3
> 
> bart <- list(preds = bartPreds, 
+ 	     lower = bartLower, 
+ 	     upper = bartUpper,
+ 	     time = bartTime)
> saveRDS(bart, "results/flood_results_bart.RDS")
> 
> ##############
> #### NNGP ####
> ##############
> 
> nIter <- 2500
> cov.model <- "exponential"
> starting <- list("phi"=5, "sigma.sq"=1, "tau.sq"=0.2)
> tuning <- list("phi"=5, "sigma.sq"=2, "tau.sq"=2)
> priors <- list("phi.Unif"=c(1,20), "sigma.sq.IG"=c(1, 1), "tau.sq.IG"=c(1, 1))
> 
> # Remove global attributes from X train and test
> X <- lapply(storms, \(i) coords$elev_meters)
> XTest <- lapply(stormsTest, \(i) coords$elev_meters)
> 
> cl <- makeCluster(nCores)
> registerDoParallel(cl)
> strt <- Sys.time()
> set.seed(mySeed)
> nngp_obj <- foreach(i = 1:nSubj, .packages = "spNNGP") %dopar% spNNGP(Y[[i]] ~ X[[i]], coords=S, 
+                                                                      starting=starting, method="latent", 
+                                                                      n.neighbors=10, tuning=tuning, 
+                                                                      priors=priors, cov.model=cov.model,
+                                                                      n.samples=nIter, n.omp.threads=1, fit.rep=TRUE)
> nngpTime <- Sys.time() - strt
> stopCluster(cl)
> if (file.exists(".RData")) {
+   file.remove(".RData")
+ }
> gc()
             used    (Mb) gc trigger    (Mb)   max used    (Mb)
Ncells     496639    26.6    1584974    84.7    1584974    84.7
Vcells 1701557618 12981.9 2362685802 18025.9 1701596916 12982.2
> 
> # Extract parameter estimates and credible intervals
> means <- apply(sapply(1:nSubj, function(i) apply(nngp_obj[[i]]$p.theta.samples, 2, mean)), 1, mean)
> lowers <- apply(sapply(1:nSubj, function(i) apply(nngp_obj[[i]]$p.theta.samples, 2, quantile, .025)), 1, mean)
> uppers <- apply(sapply(1:nSubj, function(i) apply(nngp_obj[[i]]$p.theta.samples, 2, quantile, .975)), 1, mean)
> betas <- sapply(1:nSubj, function(i) mean(nngp_obj[[i]]$p.beta.samples[ ,2]))
> nngpParams <- as.data.frame(rbind(means, lowers, uppers))
> nngpParams <- data.frame(nngpParams, beta = c(mean(betas), quantile(betas, c(.025, .975))))
> rownames(nngpParams) <- c("mean", "lower", "upper")
> 
> # Aggregate predictions for test points
> nngpPreds <- apply(sapply(1:nTestSubj, \(i) nngp_obj[[i]]$y.hat.quant[indexTest, 1]), 1, mean)
> nngpLower <- apply(sapply(1:nTestSubj, \(i) nngp_obj[[i]]$y.hat.quant[indexTest, 2]), 1, mean)
> nngpUpper <- apply(sapply(1:nTestSubj, \(i) nngp_obj[[i]]$y.hat.quant[indexTest, 3]), 1, mean)
> 
> nngp <- list(params = nngpParams, 
+ 	     preds = nngpPreds,
+ 	     lower = nngpLower,
+ 	     upper = nngpUpper,
+ 	     time = nngpTime) 
> saveRDS(nngp, "results/flood_results_nngp.RDS")
> 
> 
> ##############
> #### BASS ####
> ##############
> 
> nStorms <- length(storms) + length(stormsTest)
> nTestSubj <- length(stormsTest)
> n <- nrow(coords)
> inputs <- inputs[1:nStorms, ]
> out <- out[1:nStorms, ]
> gc()
             used    (Mb) gc trigger    (Mb)   max used    (Mb)
Ncells     502984    26.9    1584974    84.7    1584974    84.7
Vcells 1503178162 11468.4 2362685802 18025.9 1702942296 12992.5
> 
> strt <- Sys.time()
> set.seed(mySeed)
> model <- bassPCA(inputs[-stormsTest, ], out[-stormsTest, ], n.pc=2, n.cores=1)
MCMC Start #-- Oct 24 02:41:44 AM --# nbasis: 0 
MCMC iteration 1000 #-- Oct 24 02:41:44 AM --# nbasis: 1 
MCMC iteration 2000 #-- Oct 24 02:41:45 AM --# nbasis: 1 
MCMC iteration 3000 #-- Oct 24 02:41:45 AM --# nbasis: 2 
MCMC iteration 4000 #-- Oct 24 02:41:46 AM --# nbasis: 2 
MCMC iteration 5000 #-- Oct 24 02:41:46 AM --# nbasis: 1 
MCMC iteration 6000 #-- Oct 24 02:41:47 AM --# nbasis: 2 
MCMC iteration 7000 #-- Oct 24 02:41:47 AM --# nbasis: 2 
MCMC iteration 8000 #-- Oct 24 02:41:48 AM --# nbasis: 2 
MCMC iteration 9000 #-- Oct 24 02:41:48 AM --# nbasis: 1 
MCMC iteration 10000 #-- Oct 24 02:41:49 AM --# nbasis: 1 
MCMC Start #-- Oct 24 02:41:49 AM --# nbasis: 0 
MCMC iteration 1000 #-- Oct 24 02:41:49 AM --# nbasis: 1 
MCMC iteration 2000 #-- Oct 24 02:41:50 AM --# nbasis: 1 
MCMC iteration 3000 #-- Oct 24 02:41:50 AM --# nbasis: 0 
MCMC iteration 4000 #-- Oct 24 02:41:51 AM --# nbasis: 1 
MCMC iteration 5000 #-- Oct 24 02:41:51 AM --# nbasis: 1 
MCMC iteration 6000 #-- Oct 24 02:41:52 AM --# nbasis: 1 
MCMC iteration 7000 #-- Oct 24 02:41:52 AM --# nbasis: 0 
MCMC iteration 8000 #-- Oct 24 02:41:53 AM --# nbasis: 1 
MCMC iteration 9000 #-- Oct 24 02:41:53 AM --# nbasis: 2 
MCMC iteration 10000 #-- Oct 24 02:41:54 AM --# nbasis: 0 
> predictions <- predict(model, inputs[stormsTest, ])
> bassPreds <- apply(predictions, 2:3, mean)
> bassLower <- apply(predictions, 2:3, quantile, 0.025)
> bassUpper <- apply(predictions, 2:3, quantile, 0.975)
> bassTime <- Sys.time() - strt
> 
> bass <- list(preds = bassPreds,
+ 	     lower = bassLower,
+ 	     upper = bassUpper,
+ 	     time = bassTime)
> saveRDS(bass, "results/flood_results_bass.RDS")
> 
> mspe <- sapply(1:nTestSubj, \(i) mean((bassPreds[i, indexTest] - out[stormsTest[i], indexTest])^2) )
> mean(mspe)
[1] 6.658316
> 
> pct <- sapply(1:nTestSubj, \(i) mean((bassPreds[i, indexTest] > 4) == (out[stormsTest[i], indexTest] > 4)) )
> 1-mean(pct)
[1] 0.206841
> 
> proc.time()
     user    system   elapsed 
20083.511    74.248 21450.572 

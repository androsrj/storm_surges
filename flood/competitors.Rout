
R version 4.2.0 (2022-04-22) -- "Vigorous Calisthenics"
Copyright (C) 2022 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> rm(list = ls())
> gc()
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 273690 14.7     660857 35.3   451905 24.2
Vcells 459126  3.6    8388608 64.0  1800280 13.8
> 
> # Libraries
> library(parallel) # For parallel computation
> library(doParallel) # For parallel computation
Loading required package: foreach
Loading required package: iterators
> library(foreach) # For parallel computation
> library(BayesTree) # For BART
> library(spNNGP) # For NNGP
Loading required package: coda
Loading required package: Formula
Loading required package: RANN
> library(BASS) # For BASS
> 
> # Read in
> load("data/flood_data.RData")
> 
> # Clusters and seed
> nCores <- 5
> mySeed <- 9999
> 
> # Read in indices for test data
> n <- nrow(coords)
> indexTest <- readRDS("results/test_points.RDS")
> nTest <- length(indexTest)
> storms <- 1:5
> nSubj <- length(storms)
> stormsTest <- 6:10
> nTestSubj <- length(stormsTest)
> 
> Y <- lapply(storms, \(i) out[i, ])
> X <- lapply(storms, \(i) {
+   Xstorm <- matrix(rep(unlist(inputs[i, ]), n), ncol = 5, byrow = TRUE)
+   Xelev <- coords$elev_meters #[indexTrain]
+   X <- cbind(Xstorm, Xelev)
+   colnames(X) <- c(colnames(inputs), "elev")
+   return(X)
+ })
> S <- coords[ , 1:2]
> 
> YTest <- lapply(stormsTest, \(i) out[i, indexTest])
> XTest <- lapply(stormsTest, \(i) {
+   Xstorm <- matrix(rep(unlist(inputs[i, ]), nTest), ncol = 5, byrow = TRUE)
+   Xelev <- coords$elev_meters[indexTest]
+   X <- cbind(Xstorm, Xelev)
+   colnames(X) <- c(colnames(inputs), "elev")
+   return(X)
+ })
> STest <- coords[indexTest, 1:2]
> 
> ##############
> #### BART ####
> ##############
> 
> strt <- Sys.time()
> bart_obj <- bart(do.call('rbind', X), do.call('c', Y), do.call('rbind', XTest))


Running BART with numeric y

number of trees: 200
Prior:
	k: 2.000000
	degrees of freedom in sigma prior: 3
	quantile in sigma prior: 0.900000
	power and base for tree prior: 2.000000 0.950000
	use quantiles for rule cut points: 0
data:
	number of training observations: 248595
	number of test observations: 4970
	number of explanatory variables: 6


Cutoff rules c in x<=c vs x>c
Number of cutoffs: (var: number of possible c):
(1: 100) (2: 100) (3: 100) (4: 100) (5: 100) 
(6: 100) 


Running mcmc loop:
iteration: 100 (of 1100)
iteration: 200 (of 1100)
iteration: 300 (of 1100)
iteration: 400 (of 1100)
iteration: 500 (of 1100)
iteration: 600 (of 1100)
iteration: 700 (of 1100)
iteration: 800 (of 1100)
iteration: 900 (of 1100)
iteration: 1000 (of 1100)
iteration: 1100 (of 1100)
time for loop: 27565

Tree sizes, last iteration:
2 2 2 2 2 1 2 2 3 2 3 3 2 2 3 2 2 1 2 2 
4 3 2 2 2 2 2 3 2 2 2 6 2 2 2 2 2 4 3 3 
2 4 1 3 2 3 3 3 2 2 2 4 4 2 2 2 2 2 2 3 
3 2 2 2 2 4 2 3 2 5 2 3 2 2 4 2 2 2 2 4 
2 2 4 2 2 1 2 2 3 2 2 2 2 2 1 3 2 3 2 3 
2 2 2 2 2 2 2 3 2 2 2 2 3 2 7 2 1 4 2 1 
3 4 2 4 3 2 2 2 3 2 2 3 2 4 2 1 4 2 3 3 
2 2 2 3 2 2 2 2 2 2 3 2 2 1 2 2 2 5 1 3 
2 2 2 3 2 3 2 3 2 2 2 2 2 2 2 3 3 5 2 2 
2 2 2 2 3 4 2 3 2 3 2 2 2 2 2 2 4 2 4 2 
Variable Usage, last iteration (var:count):
(1: 42) (2: 30) (3: 37) (4: 33) (5: 34) 
(6: 105) 
DONE BART 11-2-2014

> bartPreds <- bart_obj$yhat.test.mean
> bartLower <- apply(bart_obj$yhat.test, 2, quantile, 0.025)
> bartUpper <- apply(bart_obj$yhat.test, 2, quantile, 0.975)
> 
> bartTime <- Sys.time() - strt
> gc()
            used   (Mb) gc trigger    (Mb)   max used   (Mb)
Ncells    427517   22.9    1584974    84.7    1584974   84.7
Vcells 455719348 3476.9 1323883198 10100.5 1025544321 7824.3
> 
> bart <- list(preds = bartPreds, 
+ 	     lower = bartLower, 
+ 	     upper = bartUpper,
+ 	     time = bartTime)
> saveRDS(bart, "results/flood_results_bart.RDS")
> gc()
            used   (Mb) gc trigger    (Mb)   max used   (Mb)
Ncells    427763   22.9    1584974    84.7    1584974   84.7
Vcells 455719899 3476.9 1323883198 10100.5 1025544321 7824.3
> 
> 
> ##############
> #### NNGP ####
> ##############
> 
> nIter <- 2500
> cov.model <- "exponential"
> starting <- list("phi"=5000, "sigma.sq"=1, "tau.sq"=0.2)
> tuning <- list("phi"=100, "sigma.sq"=2, "tau.sq"=2)
> priors <- list("phi.Unif"=c(1,10000), "sigma.sq.IG"=c(1, 1), "tau.sq.IG"=c(1, 1))
> 
> # Remove global attributes from X train and test
> X <- lapply(storms, \(i) coords$elev_meters)
> XTest <- lapply(stormsTest, \(i) coords$elev_meters)
> 
> cl <- makeCluster(nCores)
> registerDoParallel(cl)
> strt <- Sys.time()
> set.seed(mySeed)
> nngp_obj <- foreach(i = 1:nSubj, .packages = "spNNGP") %dopar% spNNGP(Y[[i]] ~ X[[i]], coords=S, 
+                                                                      starting=starting, method="latent", 
+                                                                      n.neighbors=10, tuning=tuning, 
+                                                                      priors=priors, cov.model=cov.model,
+                                                                      n.samples=nIter, n.omp.threads=1, fit.rep=TRUE)
> nngpTime <- Sys.time() - strt
> stopCluster(cl)
> gc()
             used    (Mb) gc trigger    (Mb)   max used    (Mb)
Ncells     496637    26.6    1584974    84.7    1584974    84.7
Vcells 1701557618 12981.9 2362685795 18025.9 1701596901 12982.2
> 
> # Extract parameter estimates and credible intervals
> means <- apply(sapply(1:nSubj, function(i) apply(nngp_obj[[i]]$p.theta.samples, 2, mean)), 1, mean)
> lowers <- apply(sapply(1:nSubj, function(i) apply(nngp_obj[[i]]$p.theta.samples, 2, quantile, .025)), 1, mean)
> uppers <- apply(sapply(1:nSubj, function(i) apply(nngp_obj[[i]]$p.theta.samples, 2, quantile, .975)), 1, mean)
> betas <- sapply(1:nSubj, function(i) mean(nngp_obj[[i]]$p.beta.samples[ ,2]))
> nngpParams <- as.data.frame(rbind(means, lowers, uppers))
> nngpParams <- data.frame(nngpParams, beta = c(mean(betas), quantile(betas, c(.025, .975))))
> rownames(nngpParams) <- c("mean", "lower", "upper")
> 
> # Aggregate predictions for test points
> nngpPreds <- apply(sapply(1:nTestSubj, \(i) nngp_obj[[i]]$y.hat.quant[indexTest, 1]), 1, mean)
> nngpLower <- apply(sapply(1:nTestSubj, \(i) nngp_obj[[i]]$y.hat.quant[indexTest, 2]), 1, mean)
> nngpUpper <- apply(sapply(1:nTestSubj, \(i) nngp_obj[[i]]$y.hat.quant[indexTest, 3]), 1, mean)
> 
> nngp <- list(params = nngpParams, 
+ 	     preds = nngpPreds,
+ 	     lower = nngpLower,
+ 	     upper = nngpUpper,
+ 	     time = nngpTime) 
> saveRDS(nngp, "results/flood_results_nngp.RDS")
> 
> 
> ##############
> #### BASS ####
> ##############
> 
> nStorms <- length(storms) + length(stormsTest)
> nTestSubj <- length(stormsTest)
> n <- nrow(coords)
> inputs <- inputs[1:nStorms, ]
> out <- out[1:nStorms, ]
> gc()
             used    (Mb) gc trigger    (Mb)   max used    (Mb)
Ncells     502982    26.9    1584974    84.7    1584974    84.7
Vcells 1503178162 11468.4 2362685795 18025.9 1702942296 12992.5
> 
> bassParallel <- function(stormsTest) {
+   obj <- bassPCA(inputs[-stormsTest, ], out[-stormsTest, ], n.pc=3, n.cores=1)
+   predictions <- predict(obj, inputs[stormsTest, ])[ , , indexTest]
+   preds <- apply(predictions, 2:3, mean)
+   quants <- apply(predictions, 2:3, quantile, c(0.025, 0.975))
+   return(list(preds = preds, lower = quants[1, , ], upper = quants[2, , ]))
+ }
> 
> cl <- makeCluster(nCores)
> registerDoParallel(cl)
> strt <- Sys.time()
> nReps <- 100
> set.seed(mySeed)
> predictions <- foreach(i=1:nReps, .packages = "BASS") %dopar% bassParallel(stormsTest)
> stopCluster(cl)
> 
> dim(predictions[[1]]$preds)
[1]   5 994
> dim(predictions[[1]]$lower)
[1]   5 994
> dim(predictions[[1]]$upper)
[1]   5 994
> 
> bassPreds <- bassLower <- bassUpper <- matrix(0, nrow = nTestSubj, ncol = nTest)
> for (j in 1:nReps) {
+   bassPreds <- bassPreds + predictions[[j]]$preds
+   bassLower <- bassLower + predictions[[j]]$lower
+   bassUpper <- bassUpper + predictions[[j]]$upper
+ }
> 
> bassPreds <- bassPreds / nReps
> bassUpper <- bassUpper / nReps
> bassLower <- bassLower / nReps
> bassTime <- Sys.time() - strt
> 
> #mspe <- sapply(1:nTestSubj, \(i) mean((bassPreds[i, ] - out[stormsTest[i], indexTest])^2) )
> #print(mean(mspe))
> #pct <- sapply(1:nTestSubj, \(i) mean((bassPreds[i, ] > 4) == (out[stormsTest[i], indexTest] > 4)) )
> #print(1-mean(pct))
> 
> bass <- list(preds = bassPreds,
+              lower = bassLower,
+              upper = bassUpper,
+              time = bassTime)
> saveRDS(bass, "results/flood_results_bass.RDS")
> 
> rm(list=ls())
> gc()
         used (Mb) gc trigger    (Mb)   max used    (Mb)
Ncells 463452 24.8    1584974    84.7    1584974    84.7
Vcells 895656  6.9 1890148636 14420.7 1702942296 12992.5
> if (file.exists(".RData")) {
+   remove(".RData")
+ }
Warning message:
In remove(".RData") : object '.RData' not found
> 
> proc.time()
     user    system   elapsed 
26995.780   629.802 29470.428 

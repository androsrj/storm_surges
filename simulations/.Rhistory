setwd("C:/Users/andro/OneDrive/Desktop")
# Calculate "nearest neighbors" (m of them) for each point
library(dbscan)
m <- 10
neighbor_ids <- kNN(coords, m)$id
Fvals <- numeric(n)
Bstar <- matrix(0, n, n)
sigma2 <- 2
tau2 <- 0.2
theta <- 2
beta <- c(2, -1)
n <- 100
coordsX <- runif(n, 0, 10)
coordsY <- runif(n, 0, 10)
coords <- cbind(coordsX, coordsY)
library(fields)
D <- rdist(coords)
dim(D)
C <- sigma2 * exp(-theta * D)
X <- matrix(c(rnorm(n), rnorm(n, 10, 2)), ncol=2)
W <- rnorm(n, 0, C)
eps <- rnorm(n, 0, 1/sqrt(tau2))
Y <- c(X %*% beta + W + eps)
# Calculate "nearest neighbors" (m of them) for each point
library(dbscan)
m <- 10
neighbor_ids <- kNN(coords, m)$id
Fvals <- numeric(n)
Bstar <- matrix(0, n, n)
for (i in 1:n) {
Nx <- unname(neighbor_ids[i,])
Bvals <- C[i, Nx] %*% C[Nx, Nx]
Fvals[i] <- C[i, i] - C[i, Nx] %*% solve(C[Nx, Nx]) %*% C[Nx, i]
Bstar[i, i] <- 1
Bstar[i, Nx] <- Bvals
}
B <- t(Bstar)
Fmat <- diag(Fvals)
SigmaInv <- t(B) %*% solve(Fmat) %*% B
dim(SigmaInv)
isSymmetric(SigmaInv)
Sigma <- solve(SigmaInv)
image(Sigma)
library(GPvecchia)
obj <- vecchia_specify(coords, m = m)
SigmaVecchia <- getMatCov(obj, C)
dim(SigmaVecchia)
ifelse(TRUE, 2, 1)
ifelse(FALSE, 2, 1)
rmvnorm(1, mu=c(2, 10), diag(2))
rmvnorm(1, mu=matrix(c(2,10,15,20), 2, 2), diag(2))
rmvnorm(2, mu=matrix(c(2,10,15,20), 2, 2), diag(2))
rnorm(2, mean=c(1,5))
setwd("~/research/storm_surges/simulations")
setwd("~/research/storm_surges/simulations")
# SOURCES
source("../mcmc_functions/mcmc.R") # Metropolis-Gibbs Sampler
source("../mcmc_functions/priors.R")
source("../mcmc_functions/jacobians.R")
source("../mcmc_functions/likelihood.R")
source("../mcmc_functions/posterior.R")
source("../other_functions/sparse.R") # For sparse GP
source("../other_functions/parallel_functions.R") # Parallel wrapper functions
source("../other_functions/helper_functions.R") # Other misc functions (not part of MCMC)
# Libraries
library(anticlust) # for balanced clustering
library(splitTools) # for stratified splitting
library(twinning) # for multiplet splitting
library(parallel) # For parallel computation
library(doParallel) # For parallel computation
library(foreach) # For parallel computation
library(fields) # Distance matrix calculation
library(mvtnorm)
library(pracma) # For sparse matrix calculation
# Number of clusters for parallel implementation
#nCores <- detectCores() / 2
nCores <- 10
mySeed <- 1234
nKnots <- 500
# Load train and test data
load("data/train.RData")
load("data/test.RData")
nSubj <- length(train$X)
n <- nrow(train$X[[1]])
nTest <- nrow(test$X[[1]])
X <- train$X
Y <- train$Y
S <- train$S
D <- train$D
XTest <- test$X
YTest <- test$Y
STest <- test$S
DTest <- test$D
dim(D)
dim(DTest)
i=1
# Sequence of values for theta to iterate over
thetaVals <- seq(1, 5, length = nCores)
mProp <- 0.05
#### FULL GAUSSIAN PROCESS ####
model <- "full_gp"
mProp <- 0.01
results <- mcmc(X = X, Y = Y, D = D, S = S,
theta = thetaVals[i],
test_subjects = test_subjects,
propSD = propSD,
nIter = 2000, nBurn = 500,
model = model,
mProp = mProp,
transform = TRUE)
propSD <- c(0.04, 0.3)
results <- mcmc(X = X, Y = Y, D = D, S = S,
theta = thetaVals[i],
test_subjects = test_subjects,
propSD = propSD,
nIter = 2000, nBurn = 500,
model = model,
mProp = mProp,
transform = TRUE)
test_subjects=1
results <- mcmc(X = X, Y = Y, D = D, S = S,
theta = thetaVals[i],
test_subjects = test_subjects,
propSD = propSD,
nIter = 2000, nBurn = 500,
model = model,
mProp = mProp,
transform = TRUE)
lapply(1:10, matrix, data= NA, nrow=2, ncol=5)
setwd("~/research/storm_surges/simulations")
# SOURCES
source("../mcmc_functions/mcmc.R") # Metropolis-Gibbs Sampler
source("../mcmc_functions/priors.R")
source("../mcmc_functions/jacobians.R")
source("../mcmc_functions/likelihood.R")
source("../mcmc_functions/posterior.R")
source("../other_functions/sparse.R") # For sparse GP
source("../other_functions/parallel_functions.R") # Parallel wrapper functions
source("../other_functions/helper_functions.R") # Other misc functions (not part of MCMC)
# Libraries
library(anticlust) # for balanced clustering
library(splitTools) # for stratified splitting
library(twinning) # for multiplet splitting
library(parallel) # For parallel computation
library(doParallel) # For parallel computation
library(foreach) # For parallel computation
library(fields) # Distance matrix calculation
library(mvtnorm)
library(pracma) # For sparse matrix calculation
# Number of clusters for parallel implementation
#nCores <- detectCores() / 2
nCores <- 10
mySeed <- 1234
nKnots <- 500
test_subjects <- 1:3
# Load train and test data
load("data/train.RData")
load("data/test.RData")
nSubj <- length(train$X)
n <- nrow(train$X[[1]])
nTest <- nrow(test$X[[1]])
X <- train$X
Y <- train$Y
S <- train$S
D <- train$D
XTest <- test$X
YTest <- test$Y
STest <- test$S
DTest <- test$D
# Sequence of values for theta to iterate over
thetaVals <- seq(1, 5, length = nCores)
propSD <- c(0.04, 0.3)
mProp <- 0.05
#### FULL GAUSSIAN PROCESS ####
model <- "full_gp"
nCores=2
mProp-.01
mProp=.01
# Parallel
cl <- makeCluster(nCores)
registerDoParallel(cl)
strt<-Sys.time()
set.seed(mySeed)
obj <- foreach(i = 1:nCores, .packages = "mvtnorm") %dopar% sketching_parallel(i)
final.time <- Sys.time() - strt
stopCluster(cl)
# Wasserstein averages of quantiles across reps
sketching_results_full_gp <- wasserstein(results = obj,
nChains = nCores,
method = "sketching",
model = "full_gp",
splitType = NULL,
time = final.time)
results=obj
#results <- vector("list", nChains)
#rootPath <- paste0("results/", method, "/", model, "/", splitType, "/rep")
#for (i in 1:nChains) {
#  results[[i]] <- readRDS(paste0(rootPath, i, ".RDS"))
#}
wassersteinAcc <- rowMeans(sapply(results, \(x) unlist(x$acceptance)))
wassersteinMeans <- rowMeans(sapply(results, \(x) unlist(x$posteriorMedians)))
wassersteinLower <- rowMeans(sapply(results, \(x) unlist(x$credLower)))
wassersteinUpper <- rowMeans(sapply(results, \(x) unlist(x$credUpper)))
predsList <- lapply(results, \(x) x$preds)
predictions <- Reduce("+", predsList) / length(predsList)
class(predsList)
length(predsList)
dim(predsList[[1]])
predsList[[1]]
class(predsList[[1]])
class(predsList[[2]])
str(predsList)
class(preds[[1]][[1]])
class(predsList[[1]][[1]])
dim(predsList[[1]][[1]])
predsList[[1]][[1]]
seq(50 + 10, 250, by = 10)
results$preds
obj$preds
obj
dim(obj[[1]]$preds)
dim(obj[[1]]$preds[[1]])
predsList <- lapply(results, \(x) x$preds)
predictions <- lapply(test_subjects, function(j) {
Reduce("+", predsList[[j]]) / length(predsList[[j]])
})
test_subjects
str(predsList)
i=1
predsList <- lapply(results, \(x) x[[i]]$preds)
results[[1]]
results[[1]]$preds
predsList <- lapply(results, \(x) x[[i]]$preds)
results[[i]]$preds
results[[2]]$preds
results[[3]]$preds
i
predsList <- lapply(results, \(x) x[[i]]$preds)
rm(predsList)
predsList <- lapply(results, \(x) x[[i]]$preds)
length(results)
results[[1]]$preds
str(results)
results[[1]]$preds
predsList <- lapply(results, \(x) x$preds[[i]])
predictions <- Reduce("+", predsList) / length(predsList)
predictions
length(predsList)
for (i in test_subjects) {
predsList <- lapply(results, \(x) x$preds[[i]])
predictions <- Reduce("+", predsList) / length(predsList)
}
predictions
length(predictions)
class(predictions)
predictions <- vector("list", length(test_subjects))
for (i in test_subjects) {
predsList <- lapply(results, \(x) x$preds[[i]])
predictions[[i]] <- Reduce("+", predsList) / length(predsList)
}
class(predictions)
length(predictions)

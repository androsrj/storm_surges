model <- "full_gp"
dir.path <- paste0("results/d_and_c/full_gp/", splitType)
if (!dir.exists(dir.path)) {
dir.create(dir.path)
}
# Parallel
cl <- makeCluster(nCores)
registerDoParallel(cl)
strt <- Sys.time()
set.seed(mySeed)
obj <- foreach(i = 1:nCores, .packages = "mvtnorm") %dopar% DC_parallel(i)
mcmc <- function(X, Y, D, S,
nSubj, theta,
model = c("full_gp", "mpp", "sparse_gpp")[1],
propSD = c(0.1, 0.1),
nIter = 1000, nBurn = 100, nThin = 2,
mProp = 0.1, transform = TRUE) {
# Save model type and theta globally
model <<- model
theta <<- theta
subjs <<- 1:nSubj
# Dimensions
n <- nrow(X[[1]])
p <- ncol(X[[1]])
# Generate phi and compress data, if desired
if (transform == TRUE) {
m <<- round(mProp * n)
phi <<- matrix(rnorm(m * n, 0, 1 / sqrt(n)), nrow = m, ncol = n)
newY <<- lapply(Y, \(y) phi %*% y)
newX <<- lapply(X, \(x) phi %*% x)
} else {
m <<- n
phi <<- diag(m)
newY <<- Y
newX <<- X
}
# If MPP model, setup to obtain partition of covariance matrix
if (model == "mpp") {
S_mpp <<- rbind(S, SKnot)
D_mpp <<- rdist(S_mpp)
}
# MCMC chain properties
nIter <- nBurn + nIter # 15 to 20 thousand ideally
# Tuning parameters for variance of each proposal distribution
# Can be user-supplied
sdSigma2 <- propSD[1]
sdTau2 <- propSD[2]
#sdTheta <- propSD[3]
#trSigma2 <- trTau2 <- trTheta <- numeric(nIter) # Transformed parameters
trSigma2 <- trTau2 <- numeric(nIter) # Transformed parameters
beta <- matrix(0, nrow = p, ncol = nIter) # Beta
#acceptSigma2 <- acceptTau2 <- acceptTheta <- 0 # Track acceptance rates
acceptSigma2 <- acceptTau2 <- 0 # Track acceptance rates
# Initial values of transformed parameters (except for beta, not transformed)
trSigma2[1] <- log(2)
trTau2[1] <- log(0.2)
#trTheta[1] <- log((3 - a) / (b - 3))
beta[ , 1] <- rep(0, p)
# Base of covariance matrix for updating sigma2 (only need to compute once)
B <<- baseVariance(theta, phi = phi, D = D)
Sigma <<- exp(trSigma2[1]) * B + exp(trTau2[1]) * diag(m)
# Initial predictions for storm 1 (and non-transformed covariance matrix)
BTest <<- exp(-theta * DTest)
SigmaTest <<- exp(trSigma2[1]) * BTest + exp(trTau2[1]) * diag(nTest)
YPreds <- matrix(0, nrow = nTest, ncol = nIter)
YPreds[ , 1] <- t(rmvnorm(1, mean = as.vector(XTest[[1]] %*% beta[ , 1]), sigma = SigmaTest))
# Run Gibbs/Metropolis for one chain
for (i in 2:nIter) {
cat(paste0("Beginning iteration ", i, ".\n"))
### Metropolis update (sigma2) ###
propTrSigma2 <- rnorm(1, mean = trSigma2[i - 1], sd = sdSigma2)
#MHratio <- logPost(propTrSigma2, trTau2[i - 1], theta, beta[ , i - 1]) -
#  logPost(trSigma2[i - 1], trTau2[i - 1], theta, beta[ , i - 1])
MHratio <- logRatioSigma2(propTrSigma2,
trSigma2[i - 1],
trTau2[i - 1],
beta[ , i - 1])
if(runif(1) < exp(MHratio)) {
trSigma2[i] <- propTrSigma2
Sigma <<- SigmaProp
acceptSigma2 <- acceptSigma2 + 1
} else {
trSigma2[i] <- trSigma2[i - 1]
}
### Metropolis update (tau2) ###
propTrTau2 <- rnorm(1, mean = trTau2[i - 1], sd = sdTau2)
#MHratio <- logPost(trSigma2[i], propTrTau2, theta, beta[ , i - 1]) -
#  logPost(trSigma2[i], trTau2[i - 1], theta, beta[ , i - 1])
MHratio <- logRatioTau2(trSigma2[i - 1],
propTrTau2,
trTau2[i - 1],
beta[ , i - 1])
if (runif(1) < exp(MHratio)) {
trTau2[i] <- propTrTau2
Sigma <<-  SigmaProp
acceptTau2 <- acceptTau2 + 1
} else {
trTau2[i] <- trTau2[i - 1]
}
### Metropolis update (theta) ###
#propTrTheta <- rnorm(1, mean = trTheta[i - 1], sd = sdTheta)
#MHratio <- logPost(trSigma2[i], trTau2[i], propTrTheta, beta[ , i - 1]) -
#  logPost(trSigma2[i], trTau2[i], trTheta[i - 1], beta[ , i - 1])
#if(runif(1) < exp(MHratio)) {
#  trTheta[i] <- propTrTheta
#  acceptTheta <- acceptTheta + 1
#} else {
#  trTheta[i] <- trTheta[i - 1]
#}
#cat("Updated theta.\n")
### Gibbs update (beta) ###
SigmaInv <- solve(Sigma)
#SigmaBeta <- (n / m) * t(newX) %*% SigmaInv %*% newX + diag(p)
SigmaBetaList <- lapply(newX, \(x) t(x) %*% SigmaInv %*% x)
SigmaBeta <- (n / m) * (Reduce("+", SigmaBetaList) + diag(p))
SigmaBetaInv <- solve(SigmaBeta)
#meanBeta <- (n / m) * SigmaBetaInv %*% t(newX) %*% SigmaInv %*% newY
meanBetaList <- lapply(subjs, \(i) t(newX[[i]]) %*% SigmaInv %*% newY[[i]])
meanBeta <- (n / m) * SigmaBetaInv %*% Reduce("+", meanBetaList)
beta[ , i] <- t(rmvnorm(1, meanBeta, SigmaBetaInv))
### Posterior predictive sampling for storm 1 ###
# Computationally expensive - only compute every 10th iteration
if (i %% 10 == 0) {
SigmaTest <- exp(trSigma2[i]) * BTest + exp(trTau2[i]) * diag(nTest)
YPreds[ , i] <- t(rmvnorm(1, mean = as.vector(XTest[[1]] %*% beta[ , i]), sigma = SigmaTest))
}
}
# Acceptance rates (for Metropolis-sampled parameters)
acceptance <- c(sigma2 = acceptSigma2,
tau2 = acceptTau2) / nIter
# Remove burn-in and perform thinning
index <- seq(nBurn + 1, nIter, by = nThin)
indexY <- seq(nBurn + 10, nIter, by = 10)
trSigma2 <- trSigma2[index]
trTau2 <- trTau2[index]
#trTheta <- trTheta[index]
beta <- beta[ , index]
YPreds <- YPreds[ , indexY]
nSamples <- length(index)
# Back-transform
sigma2 <- exp(trSigma2)
tau2 <- exp(trTau2)
#theta <- fInv(trTheta)
# Trace plots
#pdf(paste0("../paper/figures/trace_plots/trace_plots_", model, ".pdf"))
#plot(1:nSamples, sigma2, type = 'l', ylab = "Sigma2", main = "")
#plot(1:nSamples, tau2, type = 'l', ylab = "Tau2", main = "")
##plot(1:nSamples, theta, type = 'l', ylab = "Trace Plot for theta")
#plot(1:nSamples, beta[1, ], type = 'l', ylab = "Beta_1", main = "")
#plot(1:nSamples, beta[p, ], type = 'l', ylab = "Beta_p", main = "")
#dev.off()
# Posterior mean estimates (can be somewhat skewed because of back-transformations)
posteriorMeans <- list(sigma2 = mean(sigma2),
tau2 = mean(tau2),
#theta = mean(theta),
beta = apply(beta, 1, mean))
# Posterior median estimates (more accurate)
posteriorMedians <- list(sigma2 = median(sigma2),
tau2 = median(tau2),
#theta = median(theta),
beta = apply(beta, 1, median))
# 95% credible interval bounds
credLower <- list(sigma2 = quantile(sigma2, 0.025),
tau2 = quantile(tau2, 0.025),
#theta = quantile(theta, 0.025),
beta = apply(beta, 1, quantile, 0.025))
credUpper <- list(sigma2 = quantile(sigma2, 0.975),
tau2 = quantile(tau2, 0.975),
#theta = quantile(theta, 0.975),
beta = apply(beta, 1, quantile, 0.975))
# Posterior predictive results for test data
preds <- apply(YPreds, 1, quantile, c(0.025, 0.5, 0.975))
# Return results
return(list(acceptance = acceptance,
posteriorMeans = posteriorMeans,
posteriorMedians = posteriorMedians,
credLower = credLower,
credUpper = credUpper,
preds = preds,
predSamples = YPreds))
}
stopCluster(cl)
cl <- makeCluster(nCores)
registerDoParallel(cl)
strt <- Sys.time()
set.seed(mySeed)
obj <- foreach(i = 1:nCores, .packages = "mvtnorm") %dopar% DC_parallel(i)
final.time <- Sys.time() - strt
stopCluster(cl)
# Helper function to run subsets in parallel for D-and-C
DC_parallel <- function(i) {
path <- paste0("results/d_and_c/", model, "/", splitType, "/rep", i, ".RDS")
results <- mcmc(X = subsetsX[[i]],
Y = subsetsY[[i]],
D = subsetsD[[i]],
S = subsetsS[[i]],
nSubj = nSubj,
theta = runif(1, 2, 4),
propSD = c(0.1, 0.25),
nIter = 100, nBurn = 100,
model = model,
transform = FALSE)
saveRDS(results, path)
}
# Helper function to run subsets in parallel for sketching
sketching_parallel <- function(i) {
path <- paste0("results/sketching/", model, "/rep", i, ".RDS")
results <- mcmc(X = X, Y = Y, D = D,
nSubj = nSubj,
theta = thetaVals[i],
propSD = c(0.08, 0.65),
nIter = 100, nBurn = 100,
model = model,
mProp = 0.02,
transform = TRUE)
saveRDS(results, path)
}
#### SPARSE GAUSSIAN PROCESS ####
model <- "sparse_gp"
# Parallel
cl <- makeCluster(nCores)
registerDoParallel(cl)
strt <- Sys.time()
set.seed(mySeed)
obj <- foreach(i = 1:nCores, .packages = c("mvtnorm", "pracma")) %dopar% DC_parallel(i)
final.time <- Sys.time() - strt
stopCluster(cl)
#### MODIFIED PREDICTIVE PROCESS ####
model <- "mpp"
dir.path <- paste0("results/d_and_c/mpp/", splitType)
# Parallel
cl <- makeCluster(nCores)
registerDoParallel(cl)
strt<-Sys.time()
set.seed(mySeed)
obj <- foreach(i = 1:nCores, .packages = c("mvtnorm", "fields")) %dopar% DC_parallel(i)
DC_parallel(1)
dim(S)
dim(SKnot)
S_mpp <<- rbind(S, SKnot)
D_mpp <<- rdist(S_mpp)
dim(D_mpp)
nKnots
n
D_mpp[1:n, (n + 1):(n + nKnots)]
DC_parallel(1)
X = subsetsX[[i]]
i=1
X = subsetsX[[i]]
Y = subsetsY[[i]]
D = subsetsD[[i]]
S = subsetsS[[i]]
nSubj
theta=runif(1,2,4)
propSD=c(0.1,0.25)
nIter=100
nBurn=100
model
transform=False
transform=FALSE
subjs <<- 1:nSubj
# Dimensions
n <- nrow(X[[1]])
p <- ncol(X[[1]])
n
m <<- n
phi <<- diag(m)
newY <<- Y
newX <<- X
S_mpp <<- rbind(S, SKnot)
D_mpp <<- rdist(S_mpp)
dim(D_mpp)
n
# MCMC chain properties
nIter <- nBurn + nIter # 15 to 20 thousand ideally
# Tuning parameters for variance of each proposal distribution
# Can be user-supplied
sdSigma2 <- propSD[1]
sdTau2 <- propSD[2]
#trSigma2 <- trTau2 <- trTheta <- numeric(nIter) # Transformed parameters
trSigma2 <- trTau2 <- numeric(nIter) # Transformed parameters
beta <- matrix(0, nrow = p, ncol = nIter) # Beta
#acceptSigma2 <- acceptTau2 <- acceptTheta <- 0 # Track acceptance rates
acceptSigma2 <- acceptTau2 <- 0 # Track acceptance rates
# Initial values of transformed parameters (except for beta, not transformed)
trSigma2[1] <- log(2)
trTau2[1] <- log(0.2)
#trTheta[1] <- log((3 - a) / (b - 3))
beta[ , 1] <- rep(0, p)
# Base of covariance matrix for updating sigma2 (only need to compute once)
B <<- baseVariance(theta, phi = phi, D = D)
Sigma <<- exp(trSigma2[1]) * B + exp(trTau2[1]) * diag(m)
# Initial predictions for storm 1 (and non-transformed covariance matrix)
BTest <<- exp(-theta * DTest)
SigmaTest <<- exp(trSigma2[1]) * BTest + exp(trTau2[1]) * diag(nTest)
YPreds <- matrix(0, nrow = nTest, ncol = nIter)
YPreds[ , 1] <- t(rmvnorm(1, mean = as.vector(XTest[[1]] %*% beta[ , 1]), sigma = SigmaTest))
nKnots
results <- mcmc(X = subsetsX[[i]],
Y = subsetsY[[i]],
D = subsetsD[[i]],
S = subsetsS[[i]],
nSubj = nSubj,
theta = runif(1, 2, 4),
propSD = c(0.1, 0.25),
nIter = 100, nBurn = 100,
model = model,
transform = FALSE)
DC_parallel(1)
gc()
setwd("~/research/storm_surges/simulations")
# SOURCES
source("../mcmc_functions/mcmc.R") # Metropolis-Gibbs Sampler
source("../mcmc_functions/priors.R")
source("../mcmc_functions/jacobians.R")
source("../mcmc_functions/likelihood.R")
source("../mcmc_functions/posterior.R")
source("../other_functions/sparse.R") # For sparse GP
source("../other_functions/parallel_functions.R") # Parallel wrapper functions
source("../other_functions/helper_functions.R") # Other misc functions (not part of MCMC)
# Libraries
library(anticlust) # for balanced clustering
library(splitTools) # for stratified splitting
library(twinning) # for multiplet splitting
library(parallel) # For parallel computation
library(doParallel) # For parallel computation
library(foreach) # For parallel computation
library(fields) # Distance matrix calculation
library(mvtnorm)
library(pracma) # For sparse matrix calculation
# Number of clusters for parallel implementation
#nCores <- detectCores() / 2
nCores <- 8
mySeed <- 1234
nKnots <- 50
# Load train and test data
load("data/train.RData")
load("data/test.RData")
nSubj <- length(train$X)
n <- nrow(train$X[[1]])
nTest <- nrow(test$X[[1]])
X <- train$X
Y <- train$Y
S <- train$S
D <- train$D
XTest <- test$X
YTest <- test$Y
STest <- test$S
DTest <- test$D
# Create knot data
locations <- runif(nKnots * 2, floor(min(S)), ceiling(max(S)))
SKnot <- matrix(locations, nrow = nKnots, ncol = 2)
DKnot <- rdist(SKnot)
#######################################################
########### RUN MCMC FOR DIVIDE-AND-CONQUER ###########
#######################################################
DC_results_full_gp <- vector("list", 4)
DC_results_sparse_gp <- vector("list", 4)
DC_results_mpp <- vector("list", 4)
# Divide indices for each split type (excluding knot data)
indices <- vector("list", 4)
indices[[1]] <- balanced_clustering(S, nCores) # subdomains
indices[[2]] <- list2Vec(partition(indices[[1]], p = rep(1/nCores, nCores))) # stratified
indices[[3]] <- multiplet(as.data.frame(cbind(X[[1]], Y[[1]])), k = nCores) # multiplets
indices[[4]] <- list2Vec(split(sample(1:n, n, replace = FALSE), as.factor(1:nCores))) # random
splits <- c("subdomains", "stratified", "multiplets", "random")
j=1
# Create split indices for divide-and-conquer
splitType <- splits[j]
index <- indices[[j]]
subsetsX <- lapply(1:nCores, function(k) {
lapply(1:nSubj, function(s) X[[s]][which(index == k), ])
})
subsetsY <- lapply(1:nCores, function(k) {
lapply(1:nSubj, function(s) Y[[s]][which(index == k)])
})
subsetsS <- lapply(1:nCores, function(k) S[which(index == k), ])
subsetsD <- lapply(1:nCores, function(k) D[which(index == k), which(index == k)])
#### FULL GAUSSIAN PROCESS ####
model <- "full_gp"
#### MODIFIED PREDICTIVE PROCESS ####
model <- "mpp"
DC_parallel(1)
DC_parallel(1)
n
# Calculates the base of the covariance matrix for likelihood function
baseVariance <- function(theta, phi, D) {
if (model == "mpp") {
#DTrain <- D[1:nTrain, 1:nTrain]
#DKnot <- D[(nTrain + 1):nObs, (nTrain + 1):nObs]
nKnots <- nrow(DKnot)
n <- nrow(D)
DCov <- D_mpp[1:n, (n + 1):(n + nKnots)]
CTrain <- exp(- theta * diag(D))
CCov <- exp(- theta * DCov)
CKnot <- exp(- theta * DKnot)
# Modified predictive process (variance correction)
middle <- tcrossprod(CCov %*% solve(CKnot), CCov)
delta <- diag(CTrain - diag(middle))
B <- tcrossprod(phi %*% (middle + delta), phi)
} else if (model == "full_gp") {
C <- exp(- theta * D)
B <- tcrossprod(phi %*% C, phi)
} else if (model == "sparse_gp") {
C <- exp(- theta * D)
B <- tcrossprod(phi %*% C, phi)
B <- sparse(B)
} else (
stop("Model type must be either 'mpp', 'full_gp', or 'sparse_gp'.")
)
return(B)
}
DC_parallel(1)
# SOURCES
source("../mcmc_functions/mcmc.R") # Metropolis-Gibbs Sampler
source("../mcmc_functions/priors.R")
source("../mcmc_functions/jacobians.R")
source("../mcmc_functions/likelihood.R")
source("../mcmc_functions/posterior.R")
source("../other_functions/sparse.R") # For sparse GP
source("../other_functions/parallel_functions.R") # Parallel wrapper functions
source("../other_functions/helper_functions.R") # Other misc functions (not part of MCMC)
# Libraries
library(anticlust) # for balanced clustering
library(splitTools) # for stratified splitting
library(twinning) # for multiplet splitting
library(parallel) # For parallel computation
library(doParallel) # For parallel computation
library(foreach) # For parallel computation
library(fields) # Distance matrix calculation
library(mvtnorm)
library(pracma) # For sparse matrix calculation
# Number of clusters for parallel implementation
#nCores <- detectCores() / 2
nCores <- 8
mySeed <- 1234
nKnots <- 50
# Load train and test data
load("data/train.RData")
load("data/test.RData")
nSubj <- length(train$X)
n <- nrow(train$X[[1]])
nTest <- nrow(test$X[[1]])
X <- train$X
Y <- train$Y
S <- train$S
D <- train$D
XTest <- test$X
YTest <- test$Y
STest <- test$S
DTest <- test$D
# Create knot data
locations <- runif(nKnots * 2, floor(min(S)), ceiling(max(S)))
SKnot <- matrix(locations, nrow = nKnots, ncol = 2)
DKnot <- rdist(SKnot)
#######################################################
########### RUN MCMC FOR DIVIDE-AND-CONQUER ###########
#######################################################
DC_results_full_gp <- vector("list", 4)
DC_results_sparse_gp <- vector("list", 4)
DC_results_mpp <- vector("list", 4)
# Divide indices for each split type (excluding knot data)
indices <- vector("list", 4)
indices[[1]] <- balanced_clustering(S, nCores) # subdomains
indices[[2]] <- list2Vec(partition(indices[[1]], p = rep(1/nCores, nCores))) # stratified
indices[[3]] <- multiplet(as.data.frame(cbind(X[[1]], Y[[1]])), k = nCores) # multiplets
indices[[4]] <- list2Vec(split(sample(1:n, n, replace = FALSE), as.factor(1:nCores))) # random
splits <- c("subdomains", "stratified", "multiplets", "random")
#### MODIFIED PREDICTIVE PROCESS ####
model <- "mpp"
# Parallel
cl <- makeCluster(nCores)
registerDoParallel(cl)
strt<-Sys.time()
set.seed(mySeed)
obj <- foreach(i = 1:nCores, .packages = c("mvtnorm", "fields")) %dopar% DC_parallel(i)
subsetsX
# Create split indices for divide-and-conquer
splitType <- splits[j]
index <- indices[[j]]
subsetsX <- lapply(1:nCores, function(k) {
lapply(1:nSubj, function(s) X[[s]][which(index == k), ])
})
subsetsY <- lapply(1:nCores, function(k) {
lapply(1:nSubj, function(s) Y[[s]][which(index == k)])
})
subsetsS <- lapply(1:nCores, function(k) S[which(index == k), ])
subsetsD <- lapply(1:nCores, function(k) D[which(index == k), which(index == k)])
# Create split indices for divide-and-conquer
splitType <- splits[j]
j=1
# Create split indices for divide-and-conquer
splitType <- splits[j]
index <- indices[[j]]
subsetsX <- lapply(1:nCores, function(k) {
lapply(1:nSubj, function(s) X[[s]][which(index == k), ])
})
subsetsY <- lapply(1:nCores, function(k) {
lapply(1:nSubj, function(s) Y[[s]][which(index == k)])
})
subsetsS <- lapply(1:nCores, function(k) S[which(index == k), ])
subsetsD <- lapply(1:nCores, function(k) D[which(index == k), which(index == k)])
stopCluster(cl)
#### MODIFIED PREDICTIVE PROCESS ####
model <- "mpp"
cl <- makeCluster(nCores)
registerDoParallel(cl)
strt<-Sys.time()
set.seed(mySeed)
obj <- foreach(i = 1:nCores, .packages = c("mvtnorm", "fields")) %dopar% DC_parallel(i)
final.time <- Sys.time() - strt
stopCluster(cl)
